
{
    
    
    
    
        
        
        
    
        
        
        
    
        
        
            
                
            
        
        
    
        
        
        
    
        
        
        
    
        
        
        
    
    "pages": [{"date":"2025-07-02","image":"","imageAlt":"","link":"http://localhost:1313/posts/kubernetes/deploying-a-jenkins-server-to-a-kubernetes-cluster/","summary":"\u003ch1 id=\"-project-2-deploying-jenkins-on-kubernetes-with-persistent-storage-minikube\"\u003eüöÄ Project 2: Deploying Jenkins on Kubernetes with Persistent Storage (Minikube)\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003ePart of the \u003cstrong\u003e10 Kubernetes Projects in 10 Weeks\u003c/strong\u003e challenge\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eWelcome to the second project in my Kubernetes challenge! This time, we\u0026rsquo;re deploying \u003cstrong\u003eJenkins\u003c/strong\u003e‚Äîa powerful CI/CD automation server‚Äîon a \u003cstrong\u003eMinikube Kubernetes cluster\u003c/strong\u003e, and we‚Äôll focus on one crucial aspect: \u003cstrong\u003epersistent storage\u003c/strong\u003e.\u003c/p\u003e","tags":["devops","kubernetes"],"text":"üöÄ project 2: deploying jenkins on kubernetes with persistent storage (minikube) part of the 10 kubernetes projects in 10 weeks challenge\nwelcome to the second project in my kubernetes challenge! this time, we\u0026rsquo;re deploying jenkins‚Äîa powerful ci/cd automation server‚Äîon a minikube kubernetes cluster, and we‚Äôll focus on one crucial aspect: persistent storage.\nstateful apps like jenkins need to store data across pod restarts‚Äîlike pipeline configurations, plugins, and credentials. we\u0026rsquo;ll achieve that using persistent volumes (pv) and persistent volume claims (pvc) in kubernetes.\nlet‚Äôs dive in! üèä‚Äç‚ôÇÔ∏è\nüß† what you\u0026rsquo;ll learn how to deploy jenkins in a kubernetes cluster using minikube how to configure persistent volumes and persistent volume claims why persistence is essential for stateful applications like jenkins üì¶ prerequisites before you start, make sure you have the following installed:\n‚úÖ minikube ‚úÖ kubectl ‚úÖ helm (optional, but makes things easier) ‚úÖ basic familiarity with kubernetes concepts (pods, services, pvcs) ‚öôÔ∏è step 1: start your minikube cluster let\u0026rsquo;s spin up a local kubernetes cluster using minikube.\nminikube start --driver=docker ","title":"Deploying a Jenkins Server to a Kubernetes Cluster"},{"date":"2025-06-25","image":"","imageAlt":"","link":"http://localhost:1313/posts/kubernetes/deploying-a-static-website-to-a-kubernetes-cluster/","summary":"\u003cp\u003eAs part of my commitment to building mastery in kubernetes, I\u0026rsquo;m starting with something simple but foundational: deploying a static website on Kubernetes.\u003c/p\u003e\n\u003cp\u003eIt\u0026rsquo;s quite an overkill and there is almost no practical reason for hosting a static site in kubernetes, but the goal here is to get hands on experience with Kubernetes components specifically pods, deployments and services, which I consider to be the building blocks of any Kubernetes application.\u003c/p\u003e","tags":["devops","kubernetes"],"text":"as part of my commitment to building mastery in kubernetes, i\u0026rsquo;m starting with something simple but foundational: deploying a static website on kubernetes.\nit\u0026rsquo;s quite an overkill and there is almost no practical reason for hosting a static site in kubernetes, but the goal here is to get hands on experience with kubernetes components specifically pods, deployments and services, which i consider to be the building blocks of any kubernetes application.\nwhat we\u0026rsquo;ll be building we\u0026rsquo;ll deploy a static site served by caddy on a local kubernetes cluster using minikube, at the end of this project, we should be able to tell:\nwhat a pod is and how containers are run how a deployment can be used to manage pods- how to expose our application to the external world with services prerequisites docker or docker desktop minikube kubectl git docker hub account the code for this project can be found in this repo\nprepare the static website we are going to be using a custom website for this project, any static website would do but this one here is a free open source example site that we will use for this project.\nwe have to build a docker image that bundles our site and server in one package and upload it to a container registry so our cluster can then pull from there.\nwe pull the example site from the repo or you can use a simple index.html file.\ngit clone https://github.com/cloudacademy/static-website-example.git since we will be using caddy server, we need to create a configuration to tell the server how to serve our files.\nwe create a new file called caddyfile with the following content\n:80 root * /srv file_server this tells the server to act as a file server, listening on port 80 and serving files in the /srv directory.\nthen we create a dockerfile which contains commands that will help build our image.\nfrom caddy:2.7.5-alpine workdir /srv copy static-website-example /srv copy caddyfile /etc/caddy/caddyfile expose 80 cmd [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;caddy run --config /etc/caddy/caddyfile --adapter caddyfile\u0026#34;] we then build our image and push to dockerhub.\ndocker build -t k8s-static-website-example . before pushing to dockerhub we may need to authenticate\ndocker login follow the prompts to login and on successful login, push the newly built image to dockerhub\ndocker push \u0026lt;username\u0026gt;/k8s-static-website-example with these out of the way, we can focus on kubernetes next.\npods in kubernetes, pods are the smallest deployable units of compute. an application in a container runs in a pod. a pod can contain more than one container but it is usually recommended to run one container per pod.\ncontainers running in a pod share the same ip address and network namespace. to learn more about pods, please refer to the official guide and docs.\nto define a pod, you can use the following yaml file\ndeployments services ","title":"Deploying a Static Website to a Kubernetes Cluster"},{"date":"2025-06-24","image":"/images/kubernetes-101-banner2.png","imageAlt":"","link":"http://localhost:1313/posts/kubernetes/understanding-kubernetes-and-cloud-native-architecture/","summary":"\u003cp\u003eLike many backend engineers, I have spent most of my time doing CRUD, writing APIs, database queries, engaging in pointless arguments about programming languages and so on. But I began to notice that most job descriptions for senior roles expected more than just programming and database skills.\u003c/p\u003e\n\u003cp\u003eSince the rise of cloud computing, understanding infrastructure management, DevOps skills and cloud-native experience are now becoming essential for a backend engineer.\u003c/p\u003e","tags":["devops","kubernetes"],"text":"like many backend engineers, i have spent most of my time doing crud, writing apis, database queries, engaging in pointless arguments about programming languages and so on. but i began to notice that most job descriptions for senior roles expected more than just programming and database skills.\nsince the rise of cloud computing, understanding infrastructure management, devops skills and cloud-native experience are now becoming essential for a backend engineer.\nthis realization led me to take a meaningful step forward in my professional journey, learning kubernetes. i came across a cncf and andela partnership offering to train developers in kubernetes which i did not let pass me by. i got accepted into the first cohort of the program, took the lfs250 course and within four weeks obtained my kcna certification as a kubernetes noob.\nthis post is a reflection of what i learned, why it matters, and where i\u0026rsquo;m headed next.\nwhat even is cloud native before diving into kubernetes, i want to talk about cloud-native architecture. cloud-native is more than just running applications in the cloud, but it is a set of technological and architectural principles and practices geared towards cost-efficiency, reliability and faster time-to-market.\nsome of the core principles or philosophy of the cloud native architecture include:\ncontainers as a unit of deployment. loosely coupled systems with microservices as the default design pattern. employing automation and ci/cd pipelines for fast, repeatable and reliable delivery of applications. dynamic orchestration of infrastructure using tools like kubernetes. the cloud native ecosystem is vast with all kinds of tools for monitoring, networking, storage and so on, but kubernetes is at the center of it all. you can read more on cloud native here.\nwhat is kubernetes going by current trends, kubernetes can be considered as the platform for deploying and managing modern applications. kubernetes is a container orchestration platform that automates the deployment, scaling, networking and self-healing of containerized applications.\nin practice, kubernetes takes your containerized workloads and ensures they\u0026rsquo;re always running in the desired state, no matter what. if an application crashes, kubernetes restarts it. if traffic spikes, it can add replicas to manage load. if a node dies, it reschedules your workloads to healthy nodes.\ncontainers are a very convenient and lightweight way to run applications for a lot of good reasons.\nbefore containers you had to run applications on virtual machines or physical servers, running more than one application on the same server would require that these two do not clash or interfere with each other in terms of resource usage for both software and hardware.\nconsider a case where two different applications both need different versions of a system library, or one application consumes much of the available resources and cuases all other process to crashes, this is not ideal and we have to ensure that each application is isolated from the other.\nvirtual machines allow you to isolate operating systems on the same hardware, so applications running in virtual machines are isolated and resource limits and constraints can be well defined.\nthe success of virtualization technology was the ability to run multiple operating systems while using same hardware, but the down side is it the operating system overhead makes it somewhat inefficient.\nduring restarts, you have to add boot times into consideration, the storage space used to install full operating systems on disk and as different applications have different resource requirements, it would be wasteful to run a static website in a virtual machine, compared to an e-commerce website.\ncontainers are like virtual machines in the sense they isolate processes, but unlike virtual machines, containers share the same underlying host operating system kernel. this makes them very lightweight and cheap to run.\nmany organizations nowadays have multiple services, or multiple instances of the same service to spread load across these instances. managing tens of containers manually is easy, but managing containers at scale gets very messy easily and we will see why. when you deploy a containerized application, you need to setup networking, manage restarts when it crashes, update to new versions and scale your instances when demands are high.\ncontainer orchestration is a solution to this problem at scale and kubernetes is a container orchestration system. it can help manage automatic deployments, scaling, fault tolerance and management of containerized applications.\nkubernetes architecture kubernetes operates in a cluster which usually spans across multiple hosts or nodes. the cluster can be divided into two parts, the control plane and the worker plane. the control plane is the brain of the cluster, it manages container scheduling/placement to worker nodes, self-healing, deployments etc. the worker nodes run the containerized workloads.\nsome of the major concepts in kubernetes are as follows:\npods: the smallest deployable units, often one container, sometimes more.\ndeployments: manage the lifecycle of pods.\nservices: expose your pods internally or externally.\nnodes \u0026amp; clusters: infrastructure components that run and manage workloads.\ncontrol plane vs worker nodes: the brain and the muscle of the cluster.\nkubernetes is an absolute beast and is one of the most complex tools to learn in software engineering. one blog post cannot be enough to capture the depth of kubernetes. so i am going to start simple by building 10 simple projects that highlights the different parts of kubernetes.\ni started this journey to grow beyond my backend bubble‚Äîand i\u0026rsquo;m glad i did. kubernetes and the cloud-native ecosystem opened up a whole new layer of infrastructure understanding that complements my developer skills.\nif you\u0026rsquo;re a backend engineer thinking about devops, cloud-native, or just trying to stay ahead in your career, i highly recommend learning kubernetes.\n","title":"Understanding Kubernetes and Cloud Native Architecture"},{"date":"2024-12-03","image":"","imageAlt":"","link":"http://localhost:1313/posts/deploy-destroy-repeat/","summary":"\u003cp\u003eIt is week 2 of my devops project challenges and the challenge of the week was to automate the deployment and configuration of the \u003ca href=\"/posts/compose-your-apps\"\u003eprevious\u003c/a\u003e week\u0026rsquo;s challenge. It feels like the next step in the DevOps process, you have been able to package your applications, but you still need to setup infrastructure and many times repeatedly, doing this manually is not only tedious and time consuming but can lead to inconsistencies in the running environments for your applications, more importantly, it doesn\u0026rsquo;t scale.\u003c/p\u003e","tags":["ansible","automation","devops","terraform"],"text":"it is week 2 of my devops project challenges and the challenge of the week was to automate the deployment and configuration of the previous week\u0026rsquo;s challenge. it feels like the next step in the devops process, you have been able to package your applications, but you still need to setup infrastructure and many times repeatedly, doing this manually is not only tedious and time consuming but can lead to inconsistencies in the running environments for your applications, more importantly, it doesn\u0026rsquo;t scale.\nin this week\u0026rsquo;s challenge we would be using the popular iac tool terraform in conjunction with the popular automation and configuration management tool ansible to create a automated deployment process for our applictions. the goal is to be able to provision, configure and run all the necessary infrastructure required and then also pull them down when necessary, all with one command.\nterraform apply -auto-approve to achieve this, we are going to need to be familiar with a couple of tools which should be installed on our machine.\nterraform: to automate infrastructure provisioning ansible: to automate the configuration of our infrastructure and manage our application aws cli: used for aws get specific information on the state of our infrastructure bash scripting: to write custom logic during automation application overview to provide a recap of week one challenge, we packaged our apps into docker containers, consisting of a frontend appliaction, backend application, a database and host of monitoring services, then put all these services behind a reverse proxy to route traffic to the appropriate target, so all our http services are only exposed through the reverse proxy. this diagram shows the architecture of our app\nthe repo for this week\u0026rsquo;s challenge can be found here.\nterraform terraform is an open-source infrastructure as code (iac) tool that allows you to define, provision, and manage infrastructure resources across various cloud providers and on-premises environments using a high-level configuration language called hcl. we would be using terraform to create our ec2 instance and to trigger ansible to begin configuring our provisioned instance.\nbut before that, we would need to build, tag and push the docker images for our frontend and backend application to a container registry so we can pull these easily. fork the repo and clone to your local machine, copy over the files from the previous week\u0026rsquo;s challenge to our new repo run the following commands to build and push.\ndocker login # do this if you have not logged in docker build -f dockerfile.frontend . docker build -f dockerfile.backend . docker images # note the image id for your frontend and backend images docker tag \u0026lt;image_id\u0026gt; \u0026lt;dockerhub_username\u0026gt;/frontend:latest # replace \u0026lt;image_id\u0026gt; and \u0026lt;dockerhub_username\u0026gt; docker tag \u0026lt;image_id\u0026gt; \u0026lt;dockerhub_username\u0026gt;/backend:latest # replace \u0026lt;image_id\u0026gt; and \u0026lt;dockerhub_username\u0026gt; docker push \u0026lt;dockerhub_username\u0026gt;/frontend:latest docker push \u0026lt;dockerhub_username\u0026gt;/backend:latest now that we have pushed our image to dockerhub, we have to modify our docker-compose.app.yml to pull these images instead.\nservices: frontend: image: \u0026lt;dockerhub_username\u0026gt;/frontend:latest networks: - app_network backend: image: \u0026lt;dockerhub_username\u0026gt;/backend:latest networks: - app_network # ... this way, we only rely on the already built docker images, somewhat seperating our build and deploy stages. with these out of the way, we can start terraforming the chaos of infrastructure deployment.\nwe would proceed by writing hcl config files to define our infrastructure. create a main.tf file and variables.tf file. our main.tf file houses our infrastructue definitions and variables.tf contains our variables. we want to provision an ec2 instance with docker and docker-compose installed.\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;web_server\u0026#34; { ami = var.ami instance_type = \u0026#34;t2.micro\u0026#34; key_name = var.key_name vpc_security_group_ids = var.security_group_ids associate_public_ip_address = var.eip == null user_data = \u0026lt;\u0026lt;-eof #!/bin/bash curl -fssl https://get.docker.com -o get-docker.sh sudo sh get-docker.sh sudo usermod -ag docker ubuntu newgrp docker curl -sl https://github.com/docker/compose/releases/download/v2.30.3/docker-compose-linux-x86_64 -o docker-compose sudo chmod +x docker-compose sudo mv docker-compose /usr/local/bin/docker-compose eof tags = { name = \u0026#34;web_server\u0026#34; } } resource \u0026#34;aws_eip_association\u0026#34; \u0026#34;eip_assoc\u0026#34; { count = var.eip == null ? 0 : 1 instance_id = aws_instance.web_server.id allow_reassociation = true allocation_id = var.eip } in variables.tf\nvariable \u0026#34;security_group_ids\u0026#34; { description = \u0026#34;the aws security group to apply.\u0026#34; type = list(string) } variable \u0026#34;ami\u0026#34; { description = \u0026#34;the ami id to use for the instance.\u0026#34; type = string default = \u0026#34;ami-0866a3c8686eaeeba\u0026#34; } variable \u0026#34;key_name\u0026#34; { description = \u0026#34;key pair\u0026#34; type = string } variable \u0026#34;eip\u0026#34; { description = \u0026#34;optional elastic ip address to associate with instance if present\u0026#34; type = string default = null } variable \u0026#34;domain_name\u0026#34; { description = \u0026#34;the domain name to use for the application.\u0026#34; type = string } we would use environment variables to set the values for these variables and to do that, values for these variables can be gotten from your aws console e.g the ami id, key pair etc. terraform requires the tf_var_ prefix for each defined variable, we also need to set our aws iam credentials to use the aws plugin for terraform. you could put all these variables in a .env file and source it in your current terminal session to make these variables reflect.\nexport aws_access_key_id=\u0026#34;anaccesskey\u0026#34; export aws_secret_access_key=\u0026#34;asecretkey\u0026#34; export aws_region=\u0026#34;us-west-2\u0026#34; export tf_var_ami=\u0026#34;ami-eba\u0026#34; export tf_var_security_group_ids=\u0026#39;[\u0026#34;sg-eba\u0026#34;]\u0026#39; export tf_var_domain_name=\u0026#34;mydomain.com\u0026#34; export tf_var_eip=\u0026#34;eipalloc-xxxxx\u0026#34; # optional to use elastic ip the next step is to tell terraform to initialize our project by running the command:\nterraform init this command fetches all our plugins listed in the main.tf file. we can then run the plan command to understand how terraform is going to provision these resources\nterraform plan we will see a plan on what terraform will create and how it plans to do so. now we can run our apply command to provision the resources.\nterraform apply it would display a prompt asking if you want to proceed, type yes and watch magic happen. in few minutes, you should have your ec2 instance running, and you can ssh into it to confirm it is accessible. we shall the proceed to configuring our server to run our apps with ansible.\nansible we would use ansible to pull our docker images to our host platform which already has docker installed, copy over the necessary config files for our monitoring services and reverse proxy, generate free ssl certificates to enable https and start our containers.\nfirst, we need to generate an inventory file to tell ansible about our remote hosts, i.e the ec2 instance, we need the ip address of this instance and we can get that by modifying our terraform config file to include this step after provisioning the instance.\nin the main.tf file, add the following:\nresource \u0026#34;local_file\u0026#34; \u0026#34;ansible_inventory\u0026#34; { content = \u0026lt;\u0026lt;-eof [web_servers] ${var.eip != null ? aws_eip_association.eip_assoc.public_ip : aws_instance.web_server.public_ip} ansible_user=ubuntu ansible_ssh_private_key_file=devops_challenge.pem eof filename = \u0026#34;${path.module}/inventory.ini\u0026#34; file_permission = 0644 } this would create local file on our host machine named inventory.ini which would contain the ip address of our newly created ec2 instance.\n[web_servers] 34.116.202.250 ansible_user=ubuntu ansible_ssh_private_key_file=key_pair.pem make sure you have the key pair used to launch the ec2 instance saved as this would be needed when ansible wants to connect to your servers.\nrun the terraform init command again to fetch the local provider for the local_file resource and run the terraform plan \u0026amp;\u0026amp; terraform apply commands to see your new inventory file. this tells ansible what hosts are available to be configured.\nansible uses playbooks which are just yaml files to define plays which contains tasks to be run on each hosts, we would define a playbook to configure our instances but first we need to ensure our instances are up and available before ansible can begin, this is where the aws cli tool will be used to query for our instance state and wait till it is ready. modify the local_file resource like so:\nresource \u0026#34;local_file\u0026#34; \u0026#34;ansible_inventory\u0026#34; { content = \u0026lt;\u0026lt;-eof [web_servers] ${var.eip != null ? aws_eip_association.eip_assoc.public_ip : aws_instance.web_server.public_ip} ansible_user=ubuntu ansible_ssh_private_key_file=devops_challenge.pem eof filename = \u0026#34;${path.module}/inventory.ini\u0026#34; file_permission = 0644 provisioner \u0026#34;local-exec\u0026#34; { command = \u0026lt;\u0026lt;-eof aws ec2 wait instance-status-ok --instance-ids ${aws_instance.web_server.id} ansible-playbook -i inventory.ini -e \u0026#34;app_server_name=${var.domain_name}\u0026#34; ansible/deploy.yml eof } } create a new folder to house all our ansible related files and create a playbook called deploy.yml. we would be using ansible roles to organize our ansible configuration and it requires a certain folder structure.\ncreate the following files\nmkdir -p ansible/roles/web_servers/tasks mkdir ansible/roles/web_servers/files touch ansible/deploy.yml touch ansible/roles/web_servers/tasks/main.yml we just created a deploy playbook and web_server roles, roles can contain a tasks/ folder which defines the tasks the role would perform tasks in the tasks/main.yml play. the files/ folder contains files needed for our roles to run, files like the docker-compose files, config files for prometheus, grafana etc and nginx configuration would need to be copied over to the remote host. we can mv those files over to the files/ directory.\nwe begin with the deploy playbook which contains only one play:\n- hosts: web_servers become: yes roles: - web_servers the play targets the web_servers hosts that was defined in our inventory file and uses the web_servers role to run tasks on this target. the roles/web_servers/tasks/main.yml would then contain all our tasks for this play.\n- name: ensure parent directory exist file: path: \u0026#34;/home/ubuntu/app\u0026#34; state: directory mode: \u0026#34;0755\u0026#34; - name: copy files copy: src: \u0026#34;{{ item.src }}\u0026#34; dest: \u0026#34;/home/ubuntu/app/{{ item.dest }}\u0026#34; loop: - src: \u0026#34;docker-compose.app.yml\u0026#34; dest: \u0026#34;docker-compose.app.yml\u0026#34; - src: \u0026#34;docker-compose.monitoring.yml\u0026#34; dest: \u0026#34;docker-compose.monitoring.yml\u0026#34; - src: \u0026#34;prometheus.yml\u0026#34; dest: \u0026#34;prometheus.yml\u0026#34; - src: \u0026#34;loki-config.yml\u0026#34; dest: \u0026#34;loki-config.yml\u0026#34; - src: \u0026#34;promtail-config.yml\u0026#34; dest: \u0026#34;promtail-config.yml\u0026#34; - src: \u0026#34;grafana.ini\u0026#34; dest: \u0026#34;grafana.ini\u0026#34; - src: \u0026#34;nginx\u0026#34; dest: \u0026#34;.\u0026#34; - name: create .env for docker-compose to load copy: dest: \u0026#34;/home/ubuntu/app/.env\u0026#34; content: | export compose_file=docker-compose.app.yml:docker-compose.monitoring.yml export app_server_name=\u0026#34;{{ app_server_name }}\u0026#34; mode: 0644 - name: request letsencrypt ssl cert script: cmd: \u0026#34;init-certbot.sh\u0026#34; become: yes args: chdir: \u0026#34;/home/ubuntu/app\u0026#34; - name: start docker containers command: docker-compose up -d become: yes args: chdir: \u0026#34;/home/ubuntu/app\u0026#34; the tasks are pretty straight forward, since we already have docker installed on this host in our provisioning phase, we don\u0026rsquo;t need to install it with ansible. we create a folder to contain our files, copy over our files to this directory, pass the necessary environment variables, create ssl certificate for https and start our services. easy peasy üòâ.\ni have written a script for the ssl certificate creation and also nginx configuration. at this point, we are close to done. but before we proceed we need to instruct ansible of our roles and where to find them. in the root folder, create an ansible.cfg file with the following content\n[defaults] host_key_checking = false roles_path = ./ansible/roles this tells ansible about our roles and also for the sake of development, prevent strict host key checking which is a common issue when dealing with euphemeral hosts like cloud vms.\nwe can proceed to run the command to provision and configure our resources\nterraform apply -auto-approve if this runs successfully, you should be able to access your app via the value of the tf_var_domain_name variable, this setup ensures that with one command we create an ec2 instance, install docker, copy over necessary configuration files, configure nginx as a reverse proxy, request ssl certificates for https and start our containers.\nyou can destroy the resources with also one command which terminates the ec2 instance and stops all services from running.\nterraform destroy -auto-approve conclusion we have been able to see how terraform and ansible can work together to automate deployment processes, they can help speed up and streamline the process of deployments in the areas where they shine. terraform is usually used to provision infrastructure and ansible to configure them. this gives us the ability to deploy infrastructure in an immutable fashion, multiple times a day in a reproducable and deterministic manner, we get to deploy, destroy and repeat!. the full solution to this challenge is contained in my repo\n","title":"Automating Deployments with Terraform and Ansible"},{"date":"2024-11-27","image":"","imageAlt":"","link":"http://localhost:1313/posts/compose-your-apps/","summary":"\u003cp\u003eThis year I decided to venture into the realm of practical DevOps adventures and I was lucky to stumble upon a set of DevOps challenges spanning 6 weeks, where each week we battle with some unique problem-solving scenarios involving various DevOps tools and technologies. This article is a documentation of my first challenge which is \u003cem\u003eDeploying a containerized application and monitoring stack to the cloud, and configuring a reverse proxy.\u003c/em\u003e\u003c/p\u003e","tags":["devops","docker"],"text":"this year i decided to venture into the realm of practical devops adventures and i was lucky to stumble upon a set of devops challenges spanning 6 weeks, where each week we battle with some unique problem-solving scenarios involving various devops tools and technologies. this article is a documentation of my first challenge which is deploying a containerized application and monitoring stack to the cloud, and configuring a reverse proxy.\nthe goal was to deploy a full-stack application with a react frontend, fastapi backend and a postgresql database in separate containers with docker-compose, putting all http services behind a reverse proxy and managing configuration for the reverse proxy, collecting logs and container metrics and building a dashboard to visualize them.\nto achieve this, i had to learn about:\ndeploying multi-container applications with docker-compose setting up adminer for database management. configuring a reverse proxy with nginx proxy manager collecting metrics with prometheus and cadvisor aggregating logs with loki and promtail adding¬†dashboards to grafana. deploying to a vm on the cloud with a custom domain. now let\u0026rsquo;s proceed.\ndocker containers it might not be obvious, but to \u0026ldquo;dockerize\u0026rdquo; our application, we need to have docker running and use docker-compose to deploy multiple containers, one for each service. the services to be deployed include:\nfrontend service backend service postgres service adminer service nginx reverse proxy service grafana service prometheus service loki service promtail service cadvisor service all these services will be deployed on a single vm in the cloud, the services can be categorized into two stacks each with a docker-compose configuration file:\nan application stack: frontend, backend, postgres, adminer, nginx proxy manager a monitoring stack: grafana, prometheus, loki, promtail and cadvisor we begin with the application stack, we first have to build docker images for our backend and frontend services and we do that by creating two dockerfiles in our project\u0026rsquo;s root directory. the link to the project repo can be found here.\ngit clone https://github.com/the-devops-dojo/cv-challenge01.git cd cv-challenge01 touch dockerfile.backend dockerfile.frontend # dockerfile.backend from python:3.10-slim workdir /opt/dojo run apt-get update \u0026amp;\u0026amp; apt-get install -y curl run curl -ssl https://install.python-poetry.org | poetry_home=\u0026#34;/opt/poetry\u0026#34; poetry_version=1.8.3 python3 - env path=\u0026#34;/opt/poetry/bin:${path}\u0026#34; copy backend/pyproject.toml backend/poetry.lock . run poetry install env pythonpath=\u0026#34;/opt/dojo\u0026#34; copy backend/ . run curl -o wait-for-it.sh https://raw.githubusercontent.com/vishnubob/wait-for-it/master/wait-for-it.sh run chmod +x wait-for-it.sh expose 8000 cmd [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;./wait-for-it.sh -s postgres:5432 -- poetry run bash ./prestart.sh \u0026amp;\u0026amp; poetry run uvicorn --host 0.0.0.0 --port 8000 app.main:app --reload\u0026#34;] # dockerfile.frontend from node:18 as build workdir /opt/app copy frontend/package.json frontend/package-lock.json ./ run npm install copy frontend/ . env vite_api_url=\u0026#34;http://\u0026lt;localhost or vm server hostname/ip\u0026gt;\u0026#34; run npm run build from nginx:stable-alpine as production copy --from=build /opt/app/dist /usr/share/nginx/html expose 80 cmd [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] now that we have our dockerfiles ready, we need to create our docker-compose configuration file for the application stack.\ntouch docker-compose.app.yml and the content includes the following:\nservices: frontend: build: context: . dockerfile: dockerfile.frontend networks: - app_network postgres: image: postgres:16 environment: - postgres_db=challenge01 - postgres_password=pssw0rd volumes: - postgres:/var/lib/postgresql/data ports: - \u0026#34;5432:5432\u0026#34; networks: - app_network backend: build: context: . dockerfile: dockerfile.backend networks: - app_network depends_on: - postgres adminer: image: adminer restart: always networks: - app_network proxy-manager: image: \u0026#34;jc21/nginx-proxy-manager:latest\u0026#34; restart: unless-stopped ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; - \u0026#34;81:81\u0026#34; volumes: - ./data:/data - ./letsencrypt:/etc/letsencrypt networks: - app_network networks: app_network: driver: bridge volumes: postgres: driver: local modify the backend/.env file to include our database credentials, we are hardcoding these in the docker-compose file for convenience, in a production environment, this is bad practice as secrets as this should not be exposed publicly.\npostgres_server=\u0026#34;postgres\u0026#34; postgres_port=5432 postgres_db=challenge01 postgres_user=postgres postgres_password=\u0026#34;pssw0rd\u0026#34; with the enviroment variables set, we can run our docker-compose command to build and run our code. but remember we created a docker-compose.app.yml file and we will be creating another config file for the monitoring stack but the docker-compose tool uses a docker-compose.yml file as default which can be overriden by passing the -f flag to the command. so we have the option to pass this file explicitly with the -f flag or we can set an appropriate variable to tell docker-compose where to find the config files. we would be taking the latter approach.\ncreate a .env file in the project\u0026rsquo;s root directory and set the contents to the following:\ncompose_file=docker-compose.app.yml this tells docker-compose where to find the configuration files. with this setup, we can run our docker-compose command\ndocker-compose up -d --build this would build and start the services we have defined so far, all our services should be up, but how do we access them?\nreverse proxy setup a reverse proxy is a server that sits between client devices and backend servers, forwarding client requests to the appropriate server and then returning the server\u0026rsquo;s response back to the client. it acts as an intermediary, managing traffic and providing various benefits like security, scalability, and load balancing. in this challenge, we will be using nginx and nginx proxy manager to manage traffic to our containers.\nif you inspect our docker-compose.app.yml file, you would notice only our proxy-manager service has the host-container ports mapped, 80 for http, 443 for https and 81 is an admin interface for nginx proxy manager, this is the only http service we are exposing to the internet, traffic would be routed internally through our reverse proxy.\nnavigate to http://localhost:81 (or your server\u0026rsquo;s ip/dns hostname on port 81) and you should see the admin login interface.\nafter logging in for the first time you are asked to input a new password and email, then you should be able to set proxy rules to route traffic from our proxy-manager container to other containers since they are in the same network according to our config file.\nclick on \u0026ldquo;proxy hosts\u0026rdquo; to begin configuring proxy setting for your services.\nto add a proxy host configuration, click on the \u0026ldquo;add proxy host\u0026rdquo; button and it should bring up\nput your server domain name in the \u0026ldquo;domain names\u0026rdquo; input, and put the value proxy-manager as defined in our compose config as the forward destination on port 80, so every request is forwarded to port 80, then navigate to the \u0026ldquo;custom locations\u0026rdquo; tab to set routing rules.\nfor each entry in the table below, click on \u0026ldquo;add location\u0026rdquo; to define the following routing rules.\nlocation forward hostname/ip port / frontend 80 /api backend 8000 /docs backend/api/v1/openapi.json 8000 /prometheus prometheus 9090 /grafana grafana 3000 /loki loki 3100 /promtail promtail 9080 /cadvisor cadvisor 8080 we would come back to the rest of the services, let us focus on the application stack, if everything goes well, you should be able to visit your server\u0026rsquo;s ip/hostname and see the frontend application, log in with the credentials in the backend/.env file and voila! you are a seasoned devops engineer.\nsome routes may need further configuration especially with handling redirection, so you may need to add some custom nginx configuration by clicking on the setting icon for the specific location and adding the following\nlocation /\u0026lt;location\u0026gt; { rewrite ^/\u0026lt;location\u0026gt;/(.*)$ /$1 break; proxy_pass \u0026lt;container-name\u0026gt;:\u0026lt;container-port\u0026gt;/; proxy_redirect / /\u0026lt;location\u0026gt;/; } replace location with the appropriate location you are defining and the corresponding container-name and container-port. e.g\nlocation /prometheus { rewrite ^/prometheus/(.*)$ /$1 break; proxy_pass http://cv-challenge01-prometheus-1:9090/; proxy_redirect / /prometheus/; } this helps fix some common redirection issues, particularly for services like prometheus, and grafana.\nfinally create another proxy host, this time for the adminer service, similar to how we routed all requests from our domain through our proxy-manager container, we would route a subdomain db.\u0026lt;domain\u0026gt; to the adminer container.\nmonitoring and logging to setup our monitoring stack, we create a docker-compose.monitoring.yml file to define our services\ntouch docker-compose.monitoring.yml services: prometheus: image: prom/prometheus:latest restart: always volumes: - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro command: - \u0026#34;--config.file=/etc/prometheus/prometheus.yml\u0026#34; networks: - app_network grafana: image: grafana/grafana:latest restart: always environment: - \u0026#34;gf_install_plugins=https://storage.googleapis.com/integration-artifacts/grafana-lokiexplore-app/grafana-lokiexplore-app-latest.zip;grafana-lokiexplore-app\u0026#34; volumes: - grafana-storage:/var/lib/grafana - ./grafana.ini:/etc/grafana/grafana.ini:ro networks: - app_network loki: image: grafana/loki:latest command: -config.file=/etc/loki/config.yml volumes: - ./loki-config.yml:/etc/loki/config.yml networks: - app_network promtail: image: grafana/promtail:latest volumes: - /var/log:/var/log - /var/lib/docker/containers:/var/lib/docker/containers:ro # for wsl2 docker volume see more (https://github.com/vacp2p/wakurtosis/issues/58) - ./promtail-config.yml:/etc/promtail/config.yml command: \u0026#34;-config.file=/etc/promtail/config.yml\u0026#34; networks: - app_network cadvisor: image: gcr.io/cadvisor/cadvisor:latest volumes: - /:/rootfs:ro - /var/run:/var/run:rw - /sys:/sys:ro - /var/lib/docker/:/var/lib/docker:ro - /dev/disk:/dev/disk:ro networks: - app_network volumes: grafana-storage: driver: local we also need to setup configuration files for prometheus, grafana, loki and promtail. so create the following files\ntouch prometheus.yml grafana.ini loki-config.yml promtail-config.yml # prometheus.yml global: scrape_interval: 15s evaluation_interval: 15s scrape_configs: - job_name: \u0026#34;prometheus\u0026#34; static_configs: - targets: - \u0026#34;localhost:9090\u0026#34; - job_name: \u0026#34;loki\u0026#34; static_configs: - targets: - \u0026#34;loki:3100\u0026#34; - job_name: \u0026#34;promtail\u0026#34; static_configs: - targets: - \u0026#34;promtail:9080\u0026#34; - job_name: cadvisor scrape_interval: 5s static_configs: - targets: - \u0026#34;cadvisor:8080\u0026#34; # grafana.ini [server] # url to serve grafana from, it is important that this matches the url used by the client. root_url = %(protocol)s://%(domain)s:%(http_port)s/grafana/ # loki-config.yml auth_enabled: false server: http_listen_port: 3100 grpc_listen_port: 9096 common: instance_addr: 127.0.0.1 path_prefix: /tmp/loki storage: filesystem: chunks_directory: /tmp/loki/chunks rules_directory: /tmp/loki/rules replication_factor: 1 ring: kvstore: store: inmemory query_range: results_cache: cache: embedded_cache: enabled: true max_size_mb: 100 schema_config: configs: - from: 2020-10-24 store: tsdb object_store: filesystem schema: v13 index: prefix: index_ period: 24h ruler: alertmanager_url: http://localhost:9093 # by default, loki will send anonymous, but uniquely-identifiable usage and configuration # analytics to grafana labs. these statistics are sent to https://stats.grafana.org/ # # statistics help us better understand how loki is used, and they show us performance # levels for most users. this helps us prioritize features and documentation. # for more information on what\u0026#39;s sent, look at # https://github.com/grafana/loki/blob/main/pkg/analytics/stats.go # refer to the buildreport method to see what goes into a report. # # if you would like to disable reporting, uncomment the following lines: #analytics: # reporting_enabled: false pattern_ingester: enabled: true limits_config: allow_structured_metadata: true volume_enabled: true # promtail-config.yml server: http_listen_port: 9080 grpc_listen_port: 0 positions: filename: /tmp/positions.yaml clients: - url: http://loki:3100/loki/api/v1/push scrape_configs: - job_name: system static_configs: - targets: - localhost labels: job: varlogs __path__: /var/log/*log - job_name: containers static_configs: - targets: - localhost labels: job: \u0026#34;containerlogs\u0026#34; __path__: /var/lib/docker/containers/*/*.log edit the .env file in the root folder like so:\ncompose_file=docker-compose.app.yml:docker-compose.monitoring.yml now we are telling docker-compose to use these two config files to run our containers. we can now run our containers with the command\ndocker-compose up -d these should start all our services. if all goes well, you should be able to access the routes defined earlier in the reverse proxy. if any issues confirm the services are up or play around with the proxy manager custom location settings\nnavigate to the /grafana route and login with the credentials admin for both username and password, you will be prompted to change this after logging in.\nopen the menu and navigate to \u0026ldquo;connections\u0026rdquo; and select \u0026ldquo;data sources\u0026rdquo;, add loki and prometheus as data sources, once configured with the appropriate urls, we can proceed to create a dashboard.\nto save time, we can import community made dashboards like this one or you can create your own dashboards\nthe dashboard above uses metrics from prometheus exported by cadvisor to give insights on container resource usage. similarly you can set up dashboards with your loki source to stream and visualize log data.\nthat\u0026rsquo;s about it for our monitoring stack, right now we have been able to setup our application and monitoring stacks, but we need to deploy this to a vm so it is publicly available.\ncloud deployment to make our application available to the public, we need a domain name, you can get a free subdomain with afraid dns, we also need our target vm with a public ip address, we can setup an ec2 server on aws with docker installed, you can put the script to install docker and add your user to a group in the user data section before launching the instance, so it runs while the server is being provisioned. also ensure the following ports are open in the security groups or firewall. ports 22, 80, 81, 443\nonce that is out of the way and we have our ip address, we should point our domain name to this ip address, then we can go ahead to copy our development files over to our vm using scp. in our project\u0026rsquo;s root folder, copy everything to our vm like so\nscp -ri \u0026#34;key.pem\u0026#34; . user@hostname:/home/ubuntu ensure our docker-compose files were copied over and .env file, then run the docker-compose up command\ndocker-compose up -d --build make sure to set the reverse proxy configuration if you did it on localhost initially, then add ssl to your domain by either getting a free certificate from let\u0026rsquo;s encrypt right there in the nginx proxy manager admin interface or add a custom one. once ssl is set up and routing is working fine, we are done!. üéäüéäüéä\nto avoid cors errors when accessing the frontend, update the backend/.env to include your domain name in the backend_cors_origins variable.\nwrapping up we have been able to deploy a fullstack application and monitoring tools for our deployment with the help of docker-compose. docker-compose is a simple tool and i am aware that when running production grade apps, there are other preferred industry-standard alternatives like kubernetes, but we have been able to get a taste of what is required to take an app from code to deployment. you can find my repo with full solution here\nwe have learned to use tools like grafana, loki, promtail, prometheus and cadvisor to make our systems observable which is standard industry practice, also learned about reverse proxys and how to manage proxy configurations with a simple tool like nginx proxy manager.\nby combining these tools, we‚Äôve built a solid infrastructure that ensures real-time monitoring of our application\u0026rsquo;s health and performance. this framework enables us to promptly detect and address issues, establishing a reliable base for maintaining and scaling the workloads effectively.\n","title":"Deploying and Monitoring a Full-Stack App with Modern DevOps Tools."},{"date":"0001-01-01","image":"","imageAlt":"","link":"http://localhost:1313/about/","summary":"\u003cp\u003eHello! I\u0026rsquo;m Damilola\u003c/p\u003e\n\u003cp\u003eI write software as a backend developer, mostly with Typescript and Go. I am interested in cloud technologies and devops as well.\nYou can also find me on \u003ca href=\"https://twitter.com/0xdod\"\u003eTwitter\u003c/a\u003e, \u003ca href=\"https://github.com/0xdod\"\u003eGitHub\u003c/a\u003e, \u003ca href=\"https://linkedin.com/in/damilola-dolor97\"\u003eLinkedIn\u003c/a\u003e.\u003c/p\u003e","tags":[],"text":"hello! i\u0026rsquo;m damilola\ni write software as a backend developer, mostly with typescript and go. i am interested in cloud technologies and devops as well. you can also find me on twitter, github, linkedin.\n","title":"About"}]
}

